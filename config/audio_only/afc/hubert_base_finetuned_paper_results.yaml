# Configuration file for Hubert-Base fine-tuned. As presented in the paper: 
#Â "Leveraging Context for Multimodal Fallacy Classification in Political Debates"
#
# Purpose: This configuration defines all hyperparameters and settings required to
#          reproduce the 'audio-only' results in table 3.
#
# Usage:
#   python run_training.py --config path/to/this/config.yaml
# --------------------------------------------------------------------------------------
# General Settings
# --------------------------------------------------------------------------------------
seed: 20

# --------------------------------------------------------------------------------------
# Data Configuration
# --------------------------------------------------------------------------------------
data_module:
  data_path: data_old/
  split_key: default

  # MMUsedFallacy specific dataset configurations
  dataset:
    task_name: afc
    input_mode: AUDIO_ONLY
    with_context: false
    context_window: 0

  # Dataloader settings
  batch_size: 4

  validation_split: 0.2

  collator:
    name: audio
    processor:
      model_card: facebook/hubert-base-ls960
      params:
        max_length_seconds: 50
        sampling_rate: 16000

# --------------------------------------------------------------------------------------
# Model Configuration
# --------------------------------------------------------------------------------------
model:
  model_type: transformer
  model_card: facebook/hubert-base-ls960
  num_classes: 6
  dropout_rate: 0.1
  num_layers_to_finetune: 3
  head:
    hidden_layers: [50]

loss_function:
  name: cross_entropy
  args:
    class_weights: [0.2586882, 1.05489022, 2.28787879, 3.2030303, 4.09689922, 5.18137255]

# --------------------------------------------------------------------------------------
# Training Configuration
# --------------------------------------------------------------------------------------
trainer:
  accelerator: gpu
  devices: 1
  max_epochs: 20
  precision: bf16-mixed
  accumulate_grad_batches: 3
  deterministic: true
  benchmark: false

# --------------------------------------------------------------------------------------
# Optimizer Configuration
# --------------------------------------------------------------------------------------
optimizer:
  name: AdamW
  params:
    lr: 0.0002
    weight_decay: 0.01
    fused: true
  differential_lr:
    transformer: 0.0002
    head: 0.0002

# --------------------------------------------------------------------------------------
# Learning Rate Scheduler Configuration
# --------------------------------------------------------------------------------------
scheduler:
  name: linear
  params:
    warmup_ratio: 0.3

# --------------------------------------------------------------------------------------
# Callbacks Configuration
# --------------------------------------------------------------------------------------
callbacks:  
  model_checkpoint:
    monitor: val_f1
    mode: max

  early_stopping:
    monitor: val_f1
    mode: max
    patience: 4

# --------------------------------------------------------------------------------------
# Logging Configuration
# --------------------------------------------------------------------------------------
logger:
  wandb:
    enabled: true
    project: mmused-fallacy
